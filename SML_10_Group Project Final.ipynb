{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group project: Microsoft Malware Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Group 10:  Alejandra Zambrano, Chenxin Xie, Manoj Kumar Purushothaman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sys.setenv(LANG = \"en\")\n",
    "\n",
    "# Data processing library\n",
    "\n",
    "library(data.table)       # Data manipulation\n",
    "library(plyr)             # Data manipulation\n",
    "library(dplyr)            # Data manipulation\n",
    "library(tidyr)            # Data manipulation\n",
    "library(stringr)          # String, text processing\n",
    "library(vita)             # Quickly check variable importance\n",
    "library(dataPreparation)  # Data preparation library\n",
    "library(rlist)            # Data manipulation\n",
    "library(regclass)\n",
    "library(tibble)\n",
    "library(parallel)\n",
    "library(mlrMBO)\n",
    "\n",
    "# Machine learning library\n",
    "library(mlr)           # Machine learning framework\n",
    "library(caret)         # Data processing and machine learning framework\n",
    "library(MASS)          # LDA\n",
    "library(randomForest)  # RF\n",
    "library(gbm)           # Boosting Tree\n",
    "library(xgboost)       # XGboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"regclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.limit(size=1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data summary and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data\n",
    "#### Here we read a subset (20%) of the full dataset and working only with 447.000 observations (almost 5% of the dataset in Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_full <- read.csv('./sub_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "train_ind <- sample(seq_len(nrow(train_full)), size = 447000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train <- train_full[train_ind,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train <- remove_rownames(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can observe that the target variable is balanced\n",
    "table(train$HasDetections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats<-c(\"unique_values\",\"perc_missing_values\", \"perc_biggest_category\", \"type\")\n",
    "df<-data.frame(matrix(ncol=length(stats), nrow=length(colnames(train))))\n",
    "names(df)<-stats\n",
    "rownames(df) <- colnames(train)\n",
    "df[\"type\"] <- sapply(train, class)\n",
    "for (i in colnames(train)){\n",
    "    df[i,1] <- length(unique(train[[i]]))\n",
    "    df[i,2] <- round(sum(is.na(train[i]))/nrow(train),2)\n",
    "    df[i,3] <- round(max(table(train[i]))/nrow(train),2)\n",
    "\n",
    "}                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df <- df[order(-df[\"perc_missing_values\"],-df[\"perc_biggest_category\"]),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable DefaultBrowsersIdentifier have +90% missing values, which means that these column is useless and should be dropped. Also, other variables have +50% missing values, we will remove them.We can observe in some variables that one category contains more than +90% of the total values, we will remove these imbalanced columns, take into the account that our target variable is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf <- df[(df[\"perc_biggest_category\"]<=0.9)&(df[\"perc_missing_values\"]<=0.5),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables<-rownames(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_red<-train[,variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the variables are type numeric or integers however, looking at their unique values it seems that they could be treated as categorical variables. The variables with missing values will be filling as \"no_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_red[is.na(train_red)] <- 'no_info'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting as factor all variables except MachineIdentifier\n",
    "train_red[\"MachineIdentifier\"]<-as.character(train_red$MachineIdentifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i in colnames(train_red)){\n",
    "    if (i!= \"MachineIdentifier\"){\n",
    "        train_red[i] <- as.factor(train_red[[i]])\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(train_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will exclude the categorical variables with more than 400 levels. Computing dummy variables in these variables or performance a grouped process is computer expensive and R does not support it. The variable Census_SystemVolumeTotalCapacity could be convert into numerical, it has a lot of levels and there is not missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_list<-list()\n",
    "for (i in colnames(train_red)){\n",
    "    if (length(levels(train_red[[i]])) <400)\n",
    "     v_list=list.append(v_list,i)   \n",
    "}\n",
    "v_list<-unlist(v_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IV and DV list name\n",
    "# Dependent variable (DV)\n",
    "dv_list <- c('HasDetections')\n",
    "# Independent variable (IV)\n",
    "iv_list <- setdiff(v_list, dv_list)  # Exclude the target variable\n",
    "iv_list <- setdiff(iv_list, 'MachineIdentifier')  # Exclude the MachineIdentifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRed <- train_red[,c('MachineIdentifier','HasDetections',iv_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRed$Census_SystemVolumeTotalCapacity <- as.numeric(train_red$Census_SystemVolumeTotalCapacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(trainRed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will review the level values in case we have to make some data processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Census_InternalBatteryType, we will put all levels that contains \"li\" together (lithium), 'n/a', #, Ã¿Ã¿Ã¿Ã¿ and unkn \n",
    "# together because they refer to missing values, keep nimh as a group (Nickel), the other one will keep as 'other'\n",
    "levels(trainRed$Census_InternalBatteryType) <- c('unkn','lithm','lithm','unkn','other','unkn','unkn','other','lithm','other',\n",
    "'unkn','other','other','other','other','other','other','other','lithm','other','lithm','lithm','lithm','lithm','lithm','lithm',\n",
    "'lithm','lithm','lithm','lithm','lithm','lithm','other','unkn','nimh','other','other','other','other','other','unkn','other',\n",
    "'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Census_PowerPlatformRoleName, we will put togheter UNKNOWN and Unspecified levels\n",
    "levels(trainRed$Census_PowerPlatformRoleName) <- c('unkn','AppliancePC','Desktop','EnterpriseServer','Mobile',\n",
    "                                                    'PerformanceServer','Slate','SOHOServer','unkn','unkn','Workstation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Census_PrimaryDiskTypeName, we will put togheter UNKNOWN and Unspecified levels\n",
    "levels(trainRed$Census_PrimaryDiskTypeName) <- c('unkn','HDD','SSD','unkn','unkn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Census_ChassisTypeName, we will put togheter UNKNOWN and Unknown levels\n",
    "levels(trainRed$Census_ChassisTypeName) <- c('unkn','0','127','30','31','35','36','88','AllinOne','Blade','BladeEnclosure',\n",
    "'BusExpansionChassis','CompactPCI','Convertible','Desktop','Detachable','ExpansionChassis','HandHeld','Laptop',\n",
    "'LowProfileDesktop','LunchBox','MainServerChassis','MiniPC','MiniTower','MultisystemChassis','Notebook','Other','PizzaBox',\n",
    "'Portable','RackMountChassis','SealedCasePC','SpaceSaving','StickPC','SubChassis','SubNotebook','Tablet','Tower','unkn','unkn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Census_ActivationChannel, we will change the name of the levels for proper manipulation\n",
    "levels(trainRed$Census_ActivationChannel) <- c('OEM_DM','OEM_NONSLP','Retail','Retail_Eval','Volume_GVLK','Volume_MAK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In SmartScreen, we will put togheter some repeted variables\n",
    "levels(trainRed$SmartScreen) <- c('unkn','x01','x02','x03','unkn','Block','ExistsNotSet','off','off','off','on','on',\n",
    "                                   'Prompt','RequireAdmin','RequireAdmin','Warn','Warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(trainRed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting variables to convert into dummy, Census_IsTouchEnabled and Census_IsSecureBootEnabled are dummy already\n",
    "v_dummy<-list()\n",
    "for (i in colnames(trainRed)){\n",
    "    if (length(levels(trainRed[[i]]))>2)\n",
    "     v_dummy=list.append(v_dummy,i)   \n",
    "}\n",
    "v_dummy <- unlist(v_dummy)\n",
    "v_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy<-data.frame(trainRed[c(\"MachineIdentifier\",\"HasDetections\")])\n",
    "for (i in v_dummy){\n",
    "    temp <- trainRed[,c('MachineIdentifier',i)]\n",
    "    temp <- spread(temp,i,i,convert=TRUE)\n",
    "    temp <- temp[, c(3:(ncol(temp)))]\n",
    "    names <- colnames(temp)\n",
    "    colnames(temp) <- paste(i, names, sep='_')\n",
    "    temp[!is.na(temp)]<- 1\n",
    "    temp[is.na(temp)]<- 0\n",
    "    dummy <- cbind(dummy,temp)    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining dummy dataset with the rest of variables\n",
    "other<-trainRed[c('Census_IsTouchEnabled','Census_IsSecureBootEnabled','Census_SystemVolumeTotalCapacity')]\n",
    "train_dummy <- cbind(dummy,other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(train_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy variables as factor\n",
    "dum <- which(!names(train_dummy) %in% c('MachineIdentifier', 'HasDetections','Census_SystemVolumeTotalCapacity'))\n",
    "train_dummy[,dum]  <- lapply(train_dummy[,dum], as.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping dummy variables with less than 1000 in level 1\n",
    "drop<-c()\n",
    "for (i in dum){\n",
    "    if (sum(train_dummy[,i]==1)<1000)\n",
    "    drop<-c(drop,i)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dummy<-train_dummy[,-drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train data into train_fit, valid, test (60:20:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "\n",
    "train_idx <- caret::createDataPartition(y=train_dummy[, 'HasDetections'], p=.6, list=F)\n",
    "train_fit <- train_dummy[train_idx, ]  # Train 60%\n",
    "valid_test <- train_dummy[-train_idx, ]  # Valid + Test 40%\n",
    "\n",
    "valid_idx <- caret::createDataPartition(y=valid_test[, 'HasDetections'], p=.5, list=F)\n",
    "valid <- valid_test[valid_idx, ]  # Valid 20%\n",
    "test <- valid_test[-valid_idx, ]  # Test 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the target variable class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_fit\n",
    "ddply(train_fit, \"HasDetections\", summarise, count = length(HasDetections), \n",
    "    percentage = round(length(HasDetections)/nrow(train_fit), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaild\n",
    "ddply(valid, \"HasDetections\", summarise, count = length(HasDetections), \n",
    "    percentage = round(length(HasDetections)/nrow(valid), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ddply(test, \"HasDetections\", summarise, count = length(HasDetections), \n",
    "    percentage = round(length(HasDetections)/nrow(test), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the data columns\n",
    "for (v in colnames(train_fit)) {\n",
    "    \n",
    "    # Fix the column name\n",
    "    fix_name <- str_replace_all(v, \"[^[:alnum:] ]\", \"_\")\n",
    "    fix_name <- gsub(' +', '', fix_name) \n",
    "    \n",
    "    # Train, valid,test\n",
    "    colnames(train_fit)[colnames(train_fit) == v] <- fix_name\n",
    "    colnames(valid)[colnames(valid) == v] <- fix_name\n",
    "    colnames(test)[colnames(test) == v] <- fix_name\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert variables as numeric in training dataset to calculate correlation\n",
    "no_convert <- c('MachineIdentifier','Census_SystemVolumeTotalCapacity')\n",
    "for (i in names(train_fit)){\n",
    "    if (!(i %in% no_convert)){\n",
    "        train_fit[i]<-as.numeric(levels(train_fit[[i]])[train_fit[[i]]])\n",
    "    }\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for correlated features\n",
    "no_cor<-c(1,2)\n",
    "cor <- findCorrelation(train_fit[,-no_cor], cutoff=0.75, names = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing redundant reatures\n",
    "keep <- setdiff(colnames(train_fit),cor)\n",
    "train_fit<-train_fit[,keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FisherScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FisherScore <- function(basetable, depvar, IV_list) {\n",
    "  \"\n",
    "  This function calculate the Fisher score of a variable.\n",
    "  \n",
    "  Ref:\n",
    "  ---\n",
    "  Verbeke, W., Dejaeger, K., Martens, D., Hur, J., & Baesens, B. (2012). New insights into churn prediction in the telecommunication sector: A profit driven data mining approach. European Journal of Operational Research, 218(1), 211-229.\n",
    "  \"\n",
    "  \n",
    "  # Get the unique values of dependent variable\n",
    "  DV <- unique(basetable[, depvar])\n",
    "  \n",
    "  IV_FisherScore <- c()\n",
    "  \n",
    "  for (v in IV_list) {\n",
    "    fs <- abs((mean(basetable[which(basetable[, depvar]==DV[1]), v]) - mean(basetable[which(basetable[, depvar]==DV[2]), v]))) /\n",
    "      sqrt((var(basetable[which(basetable[, depvar]==DV[1]), v]) + var(basetable[which(basetable[, depvar]==DV[2]), v])))\n",
    "    IV_FisherScore <- c(IV_FisherScore, fs)\n",
    "  }\n",
    "  \n",
    "  return(data.frame(IV=IV_list, fisher_score=IV_FisherScore))\n",
    "}\n",
    "\n",
    "varSelectionFisher <- function(basetable, depvar, IV_list, num_select) {\n",
    "  \"\n",
    "  This function will calculate the Fisher score for all IVs and select the best\n",
    "  top IVs.\n",
    "\n",
    "  Assumption: all variables of input dataset are converted into numeric type.\n",
    "  \"\n",
    "  \n",
    "  fs <- FisherScore(basetable, depvar, IV_list)  # Calculate Fisher Score for all IVs\n",
    "  num_select <- min(num_select, ncol(basetable))  # Top N IVs to be selected\n",
    "  return(as.vector(fs[order(fs$fisher_score, decreasing=T), ][1:num_select, 'IV']))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Fisher Score for all variable\n",
    "# Get the IV and DV list\n",
    "dv_list <- c('HasDetections')  # DV list\n",
    "iv_list <- setdiff(names(train_fit), dv_list)  # IV list excluded DV\n",
    "iv_list <- setdiff(iv_list, 'MachineIdentifier')  # Excluded the MachineIdentifier\n",
    "\n",
    "fs <- FisherScore(train_fit, dv_list, iv_list)\n",
    "fs <- fs[order(-fs$fisher_score),]\n",
    "head(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 50 variables according to the Fisher Score\n",
    "best_fs_var <- varSelectionFisher(train_fit, dv_list, iv_list, num_select=50)\n",
    "head(best_fs_var, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply variable selection to the data\n",
    "# Train\n",
    "var_select <- names(train_fit)[names(train_fit) %in% best_fs_var]\n",
    "train_sel <- train_fit[, c('MachineIdentifier', var_select, 'HasDetections')]\n",
    "# Valid\n",
    "var_select <- names(valid)[names(valid) %in% best_fs_var]\n",
    "valid_sel <- valid[, c('MachineIdentifier', var_select, 'HasDetections')]\n",
    "# Test\n",
    "var_select <- names(test)[names(test) %in% best_fs_var]\n",
    "test_sel <- test[, c('MachineIdentifier', var_select, 'HasDetections')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable as factor for training the models\n",
    "train_sel$HasDetections<- as.factor(train_sel$HasDetections)\n",
    "\n",
    "# Dummy variables as factor\n",
    "dum <- which(!names(train_sel) %in% c('MachineIdentifier', 'HasDetections','Census_SystemVolumeTotalCapacity'))\n",
    "train_sel[,dum]  <- lapply(train_sel[,dum], as.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=5, predict=\"both\")\n",
    "\n",
    "# Define the model\n",
    "lg_lrn <- makeLearner(\"classif.logreg\", predict.type=\"prob\")\n",
    "\n",
    "# Define the task\n",
    "lg_task <- makeClassifTask(id=\"maleware_train\", data=train_sel[, -1], target=\"HasDetections\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "    \n",
    "           makeLogicalLearnerParam(\"model\", default = TRUE, tunable = TRUE)\n",
    ")\n",
    "\n",
    "ctrl = makeTuneControlGrid(resolution = 10L)\n",
    "\n",
    "parallelStartSocket(cpus = detectCores())\n",
    " \n",
    "lgPars <- tuneParams(lg_lrn, task = lg_task,\n",
    "                     resampling = rdesc,\n",
    "                     par.set = tune_params,\n",
    "                     control = ctrl)\n",
    " \n",
    "parallelStop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learner with tuned parameters\n",
    "tunedLG <- setHyperPars(lg_lrn, par.vals = lgPars$x)\n",
    "# Retain the model \n",
    "lgModel <- mlr::train(tunedLG, lg_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on valid data\n",
    "pred <- predict(lgModel, newdata=valid_sel[, -1])\n",
    "performance(pred, measures=mlr::auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data\n",
    "pred <- predict(lgModel, newdata=test_sel[, -1])\n",
    "performance(pred, measures=mlr::auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cross-validation\n",
    "rdesc = makeResampleDesc(\"CV\", iters=5)\n",
    "\n",
    "# Define the model\n",
    "rf_lrn <- makeLearner(\"classif.randomForest\", predict.type=\"prob\")\n",
    "\n",
    "# Define the task\n",
    "rf_task <- makeClassifTask(id=\"maleware_train\", data=train_sel[, -1], target=\"HasDetections\")\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "  makeIntegerParam(\"ntree\",lower = 50, upper = 150),\n",
    "  makeIntegerParam(\"mtry\", lower = 1, upper = 5),\n",
    "  makeIntegerParam(\"nodesize\", lower = 10, upper = 50)\n",
    ")\n",
    "ctrl = makeTuneControlRandom(maxit=20L)\n",
    "\n",
    "parallelStartSocket(cpus = detectCores())\n",
    " \n",
    "rfPars <- tuneParams(rf_lrn, task = rf_task,\n",
    "                     resampling = rdesc,\n",
    "                     par.set = tune_params,\n",
    "                     control = ctrl)\n",
    " \n",
    "parallelStop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learner with tuned parameters\n",
    "tunedRF <- setHyperPars(rf_lrn, par.vals = rfPars$x)\n",
    "# Retain the model \n",
    "rfModel <- mlr::train(tunedRF, rf_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on valid data\n",
    "pred <- predict(rfModel, newdata=valid_sel[, -1])\n",
    "performance(pred, measures=mlr::auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data\n",
    "pred <- predict(rfModel, newdata=test_sel[, -1])\n",
    "performance(pred, measures=mlr::auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "gb_lrn <- makeLearner(\"classif.xgboost\", predict.type=\"prob\")\n",
    "\n",
    "# Define the task\n",
    "gb_task <- makeClassifTask(id=\"maleware_train\", data=train_sel[, -1], target=\"HasDetections\")\n",
    "\n",
    "# Set up cross-validation\n",
    "rdesc <- makeResampleDesc(\"CV\", iters=10, predict=\"both\")\n",
    "cv_inst <- makeResampleInstance(rdesc, task = gb_task)\n",
    "\n",
    "# Set hyper parameter tuning\n",
    "tune_params <- makeParamSet(\n",
    "   makeIntegerParam(\"nrounds\", lower = 100, upper = 1000),\n",
    "  makeIntegerParam(\"max_depth\", lower = 1, upper = 15),\n",
    "  makeNumericParam(\"eta\", lower = .001, upper = .5),\n",
    "  makeNumericParam(\"lambda\", lower = -1, upper = 3, trafo = function(x) 10^x))\n",
    "                   \n",
    "# set tune control                   \n",
    "mbo.ctrl <- makeMBOControl()\n",
    "mbo.ctrl <- setMBOControlTermination(mbo.ctrl, iters = 50)\n",
    "ctrl <- mlr:::makeTuneControlMBO(mbo.control = mbo.ctrl)\n",
    "\n",
    "parallelStartSocket(cpus = detectCores())\n",
    " \n",
    "xgbstPars <- tuneParams(gb_lrn, task = gb_task,\n",
    "                     resampling = cv_inst,\n",
    "                     par.set = tune_params,\n",
    "                     control = ctrl)\n",
    " \n",
    "parallelStop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learner with tuned parameters\n",
    "tunedXGB <- setHyperPars(gb_lrn, par.vals = xgbstPars$x)\n",
    "# retain the model \n",
    "xgbModel <- mlr::train(tunedXGB, gb_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on valid data\n",
    "pred <- predict(xgbModel, newdata=valid_sel[, -1])\n",
    "performance(pred, measures=mlr::auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on test data\n",
    "pred <- predict(xgbModel, newdata=test_sel[, -1])\n",
    "performance(pred, measures=mlr::auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
